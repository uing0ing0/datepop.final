{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/_kvv4hhd1qb_h0ms_8jm1nkc0000gn/T/ipykernel_4031/1489520656.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ko_KR.UTF-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException, InvalidSessionIdException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "import re\n",
    "import locale\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.load_hotspots import load_hotspots\n",
    "from utils.haversine import haversine\n",
    "from utils.convert_str_to_number import convert_str_to_number\n",
    "from utils.is_within_date import is_within_one_month, is_within_two_weeks\n",
    "from utils.get_instagram_link import get_instagram_link\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'ko_KR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['store_id', 'name', 'category', 'is_food', 'new_store', 'instagram_link', 'instagram_post', 'instagram_follower', 'hot_spot',\n",
    "            'visitor_review_count', 'blog_review_count', 'distance_from_subway', 'on_tv', 'parking_available', 'no_kids',\n",
    "            # 'image_url 잠시 제거. 크롤링 속도 확보를 위함\n",
    "            'seoul_michelin', 'age-2030', 'gender-balance', 'on_blue_ribbon', 'running_well', 'address', 'phone', 'gps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatePopCrawler:\n",
    "    columns = ['store_id', 'name', 'category', 'is_food', 'new_store', 'instagram_link', 'instagram_post', 'instagram_follower', 'hot_spot',\n",
    "               'visitor_review_count', 'blog_review_count', 'distance_from_subway', 'on_tv', 'parking_available', 'no_kids',\n",
    "               'seoul_michelin', 'age-2030', 'gender-balance', 'on_blue_ribbon', 'image_urls', 'running_well', 'address', 'phone', 'gps']\n",
    "\n",
    "    def __init__(self, location, keyword, hotspots, is_food, crawl_new, blue_ribbon):\n",
    "        self.location = location  # 크롤링 지역 ex) 강남역\n",
    "        self.keyword = keyword  # 크롤링 키워드 ex) 맛집\n",
    "        self.search_word = location + \" \" + keyword  # 크롤링 검색어 ex) 강남역 맛집\n",
    "\n",
    "        # hotspots 정보, 나중에 location 값에 따라서 자동으로 해당 지역의 hotspot 정보를 불러오도록 수정 필요\n",
    "        self.hotspots = hotspots\n",
    "        self.crawl_new = crawl_new  # 신규매장 크롤링 여부\n",
    "        self.blue_ribbon = blue_ribbon  # location에 대한 블루리본서베이 매장 정보 리스트\n",
    "\n",
    "        self.is_food = is_food  # \"맛집\" 키워드 크롤링인지\n",
    "\n",
    "        # 크롤링 결과 변수\n",
    "        self.data = pd.DataFrame(columns=DatePopCrawler.columns)\n",
    "\n",
    "        # Iframe 전환용 xpath\n",
    "        self.search_iframe = \"\"\"//*[@id=\"searchIframe\"]\"\"\"\n",
    "        self.entry_iframe = \"\"\"//*[@id=\"entryIframe\"]\"\"\"\n",
    "        self.empty_searchIframe = \"\"\"//*[@id=\"_pcmap_list_scroll_container\"]\"\"\"\n",
    "        self.empty_entryIframe = \"\"\"//*[@id=\"app-root\"]\"\"\"\n",
    "        self.empty_root = \"\"\"//*[@id=\"root\"]\"\"\"\n",
    "\n",
    "        # 한 매장에 대한 크롤링값 저장\n",
    "        self.store_dict = None\n",
    "\n",
    "        # Chrome driver 세팅\n",
    "        self.driver = self.init_driver()\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.wait_short = WebDriverWait(self.driver, 2)\n",
    "\n",
    "    # 크롬 드라이버 설정\n",
    "    def init_driver(self):\n",
    "        # 크롤링 봇 회피\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        # options.add_argument(\"lang=ko_KR\")\n",
    "        # options.add_argument(\"--no-sandbox\")  # 샌드박스 모드 비활성화\n",
    "        # options.add_argument('--disable-setuid-sandbox')  # 추가\n",
    "        # options.add_argument('--disable-dev-shm-usage')\n",
    "        # options.add_argument('--disable-gpu')\n",
    "        # options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        # options.add_argument(\"--disable-application-cache\")\n",
    "        # options.add_argument(\"--disable-infobars\")\n",
    "        # options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--incognito\")  # 시크릿 모드\n",
    "        # options.add_argument(\"--disable-cache\")  # 캐시 비활성화\n",
    "        # options.add_experimental_option(\"prefs\", {\n",
    "        #                                 \"profile.managed_default_content_settings.images\": 2})  # 이미지 로딩 비활성화로 캐시 줄이기\n",
    "\n",
    "        # options.add_argument(\n",
    "        #     'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')\n",
    "        # options.add_argument('window-size=1920, 1080')\n",
    "\n",
    "        log_directory = os.path.join(os.getcwd(), 'log')\n",
    "        current_time = datetime.now()\n",
    "        log_file_path = os.path.join(log_directory, f\"\"\"chrome_driver_{self.location}_{\n",
    "                                     self.keyword}_{current_time}.log\"\"\")\n",
    "\n",
    "        options.add_argument(\"--enable-logging\")\n",
    "        options.add_argument(\"--v=1\")\n",
    "        options.add_argument(\"--log-level=DEBUG\")\n",
    "        options.add_argument(f\"--log-path={log_file_path}\")\n",
    "\n",
    "        # service = Service(ChromeDriverManager().install())\n",
    "        # driver = webdriver.Chrome(options=options, service=service)\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        driver.get(\"https://map.naver.com/\")\n",
    "\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # 검색어를 input field에 입력 후 클릭\n",
    "    def search_keyword(self):\n",
    "        self.move_to_default_content()\n",
    "\n",
    "        css_selector = \".input_search\"\n",
    "        elem = self.wait.until(EC.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        elem.send_keys(self.search_word)\n",
    "        time.sleep(3)\n",
    "        elem.send_keys(Keys.RETURN)\n",
    "\n",
    "    def move_to_default_content(self):\n",
    "        self.driver.switch_to.default_content()\n",
    "        self.wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, self.empty_root)))\n",
    "\n",
    "    # 한 매장의 크롤링 정보를 저장하는 변수 초기화\n",
    "    def init_dictionary(self):\n",
    "        self.store_dict = {\n",
    "            \"store_id\": None,\n",
    "            \"name\": \"\",\n",
    "            \"category\": \"\",\n",
    "            \"is_food\": self.is_food,\n",
    "            \"new_store\": self.crawl_new,\n",
    "            \"instagram_link\": None,\n",
    "            \"instagram_post\": None,\n",
    "            \"instagram_follower\": None,\n",
    "            \"hot_spot\": False,\n",
    "            \"visitor_review_count\": 0,\n",
    "            \"blog_review_count\": 0,\n",
    "            \"distance_from_subway\": None,\n",
    "            \"on_tv\": False,\n",
    "            \"parking_available\": False,\n",
    "            \"no_kids\": False,\n",
    "            \"seoul_michelin\": False,\n",
    "            \"age-2030\": None,\n",
    "            \"gender-balance\": None,\n",
    "            \"on_blue_ribbon\": None,\n",
    "            \"running_well\": None,\n",
    "            \"address\": None,\n",
    "            \"phone\": None,\n",
    "            \"gps\": {\n",
    "                \"latitude\": None,\n",
    "                \"longitude\": None,\n",
    "            },\n",
    "            \"naver_url\": None,\n",
    "            # \"image_urls\": [],\n",
    "        }\n",
    "\n",
    "    # \"새로오픈\" option 클릭\n",
    "\n",
    "    def click_new_option(self):\n",
    "        time.sleep(1)\n",
    "        self.move_to_search_iframe()\n",
    "        # \"더보기\" 버튼 클릭\n",
    "        more_xpath = \"\"\"//a[span[contains(text(),'더보기')]]\"\"\"\n",
    "        more_button = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, more_xpath)))\n",
    "        self.driver.execute_script(\"arguments[0].click()\", more_button)\n",
    "        # \"새로오픈\" 버튼 클릭\n",
    "        new_xpath = \"\"\"//a[contains(text(),'새로오픈')]\"\"\"\n",
    "        new_button = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, new_xpath)))\n",
    "        self.driver.execute_script(\"arguments[0].click()\", new_button)\n",
    "\n",
    "    # 한 매장에 대한 페이지 열기\n",
    "    def get_into_store(self, index):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            # 매장의 정보를 저장할 저장할 dictionay 변수 초기화\n",
    "            self.init_dictionary()\n",
    "            # 매장 목록이 있는 search Iframe으로 이동\n",
    "            self.move_to_search_iframe()\n",
    "\n",
    "            # 현재 페이지의 index번째 매장 클릭\n",
    "            store_xpath = f\"\"\"//*[@id=\"_pcmap_list_scroll_container\"]/ul/li[{\n",
    "                index}]//a[.//div[contains(@class, 'place_bluelink')]]\"\"\"\n",
    "            elem = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, store_xpath)))\n",
    "            self.driver.execute_script(\n",
    "                \"arguments[0].scrollIntoView(true);\", elem)\n",
    "            self.driver.execute_script(\"arguments[0].click()\", elem)\n",
    "\n",
    "            # entry Iframe에 접근하기위해 상위 frame으로 이동\n",
    "            time.sleep(2)\n",
    "            self.move_to_default_content()\n",
    "\n",
    "            # 매장 정보가 있는 entry Iframe으로 이동\n",
    "            iframe_element = self.wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, self.entry_iframe)))\n",
    "            # iframe_element = self.driver.find_element(\n",
    "            #     By.XPATH, self.entry_iframe)\n",
    "\n",
    "            # entry Iframe으로 실제로 이동하기 전에 해당 매장에 대한 url 얻기\n",
    "            iframe_src = iframe_element.get_attribute('src')\n",
    "            self.store_dict[\"naver_url\"] = iframe_src\n",
    "            # 매장의 위도-경도 / 고유 ID 얻기\n",
    "            parsed_url = urlparse(iframe_src)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            latitude = float(query_params.get('y')[0])\n",
    "            longitude = float(query_params.get('x')[0])\n",
    "            self.store_dict[\"gps\"] = {\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude\n",
    "            }\n",
    "            path_segments = parsed_url.path.split('/')\n",
    "            store_id = path_segments[2]\n",
    "            self.store_dict[\"store_id\"] = store_id\n",
    "\n",
    "            # self.driver.switch_to.window(self.driver.window_handles[2])\n",
    "            # self.driver.get(iframe_src)\n",
    "\n",
    "            # entryIframe으로 이동\n",
    "            try:\n",
    "                self.driver.switch_to.frame(iframe_element)\n",
    "                self.wait.until(EC.presence_of_element_located(\n",
    "                    (By.XPATH, self.empty_entryIframe)))\n",
    "            except TimeoutException as e:\n",
    "                print(e)\n",
    "                print(\"매장 정보 로딩 실패\")\n",
    "                return False\n",
    "\n",
    "            # \"요청하신 페이지를 찾을 수 없습니다\" -> \"새로고침\" 버튼 클릭하여 매장 정보 다시 불러오기\n",
    "            try:\n",
    "                reset_elem = self.wait_short.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"\"\"//a[contains(text(), '새로고침')]\"\"\")))\n",
    "\n",
    "                self.driver.execute_script(\"arguments[0].click()\", reset_elem)\n",
    "                print(\"새로고침 발생\")\n",
    "                return True\n",
    "            except (NoSuchElementException, TimeoutException):  # 매장 정보가 잘 불러와진 경우\n",
    "                return True\n",
    "        except StaleElementReferenceException:\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(\"get_into_store에서 에러 발생\")\n",
    "            print(e)\n",
    "            return False\n",
    "\n",
    "    # 대표사진 URL 추출 함수\n",
    "    def get_image_urls(self, max_images=10, first_try=True):\n",
    "        images_xpath = \"\"\"/html/body/div[3]/div/div/div/div[6]/div[4]/div/div/div/div/a\"\"\"\n",
    "        try:\n",
    "            self.wait.until(EC.element_to_be_clickable(\n",
    "                (By.XPATH, images_xpath)))\n",
    "            images_xpath = images_xpath + \"/img\"\n",
    "            image_elements = self.driver.find_elements(By.XPATH, images_xpath)\n",
    "            image_urls = [img.get_attribute('src')\n",
    "                          for img in image_elements][:max_images]\n",
    "            return image_urls\n",
    "        except StaleElementReferenceException:\n",
    "            # 요소가 stale 상태인 경우\n",
    "            if first_try:\n",
    "                print(\"재시도\")\n",
    "                return self.get_image_urls(first_try=False)\n",
    "            else:\n",
    "                return []\n",
    "        except TimeoutException as e:\n",
    "            print(\"이미지 URL 추출 중 에러 발생: \", e)\n",
    "            try:\n",
    "                if first_try:\n",
    "                    self.move_to_tab(\"홈\")\n",
    "                    self.move_to_tab(\"사진\")\n",
    "                    return self.get_image_urls(first_try=False)\n",
    "                else:\n",
    "                    return []\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                return []\n",
    "\n",
    "    # 한 매장 내에서 특정 탭으로 전환\n",
    "    # 홈, 리뷰, 사진, 예약 등, 탭 이름을 전달해서 사용\n",
    "\n",
    "    def move_to_tab(self, tab_name):\n",
    "\n",
    "        tab_xpath = f\"\"\"//a[@role='tab' and .//span[text()='{tab_name}']]\"\"\"\n",
    "\n",
    "        tab_element = self.driver.find_element(By.XPATH, tab_xpath)\n",
    "        self.driver.execute_script(\"arguments[0].click()\", tab_element)\n",
    "        time.sleep(2)\n",
    "\n",
    "    # 한 매장에 대한 정보 얻기\n",
    "\n",
    "    def get_store_details(self):\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 매장이 핫스팟에 위치해있는지 확인\n",
    "        # 핫스팟 여부를 먼저 확인하는 건, 최대한 매장 정보 로딩 시간을 확보하기 위해서임\n",
    "        # 핫스팟 여부는 매장 정보 로딩과 별개이기 때문에 맨 처음에 넣어도 상관없음\n",
    "        print(\"핫스팟 계산\")\n",
    "        try:\n",
    "            store_point = Point(float(self.store_dict[\"gps\"][\"longitude\"]), float(\n",
    "                self.store_dict[\"gps\"][\"latitude\"]))\n",
    "            for i in range(len(self.hotspots)):\n",
    "                polygon = self.hotspots[i][\"polygon_area\"]\n",
    "                if polygon.contains(store_point):\n",
    "                    self.store_dict[\"hot_spot\"] = True  # default = False\n",
    "                    break\n",
    "        except:\n",
    "            self.store_dict['hot_spot'] = False\n",
    "\n",
    "        print(\"3초 대기\")\n",
    "        # 매장 정보 로딩을 위한 명시적 대기\n",
    "        time.sleep(3)\n",
    "        print(\"3초 끝\")\n",
    "\n",
    "        print(\"매장 이름\")\n",
    "        # 매장 이름, 카테고리\n",
    "        try:\n",
    "            store_name_xpath = \"\"\"//*[@id=\"_title\"]/div/span\"\"\"\n",
    "            elem = self.wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, store_name_xpath)))\n",
    "\n",
    "            self.store_dict['name'] = elem[0].text\n",
    "            self.store_dict['category'] = elem[1].text\n",
    "        # 드물게 매장 이름을 크롤링하지 못하는 에러 발생\n",
    "        # 해당 매장 생략하고 다음 매장 크롤링 진행\n",
    "        except TimeoutException as e:\n",
    "            print(\"매장 이름, 카테고리 에러: \", e)\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(\"매장 이름, 카테고리 에러: \", e)\n",
    "            return False\n",
    "\n",
    "        # 신규 매장 여부\n",
    "        # 일반 크롤링일 때만 실행\n",
    "        print(\"신규매장 여부 확인\")\n",
    "        if self.store_dict[\"new_store\"] == False:\n",
    "            try:\n",
    "                new_open_xpath = \"\"\"//*[@id='_title']/div/span[contains(text(), '새로오픈')]\"\"\"\n",
    "                new_open_spans = self.driver.find_element(\n",
    "                    By.XPATH, new_open_xpath)\n",
    "\n",
    "                if new_open_spans:\n",
    "                    self.store_dict[\"new_store\"] = True\n",
    "                else:\n",
    "                    pass\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        # 방문자 리뷰, 블로그 리뷰 개수\n",
    "        print(\"네이버 리뷰\")\n",
    "        try:\n",
    "            elem_visitor = self.driver.find_element(\n",
    "                By.XPATH, value=\"//a[contains(text(), '방문자리뷰')]\")\n",
    "            visitor_review_count = int(re.findall(\n",
    "                r'\\d+', elem_visitor.text.replace(\",\", \"\"))[0])\n",
    "            self.store_dict['visitor_review_count'] = visitor_review_count\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict['visitor_review_count'] = None\n",
    "        try:\n",
    "            elem_blog = self.driver.find_element(\n",
    "                By.XPATH, value=\"//a[contains(text(), '블로그리뷰')]\")\n",
    "            blog_review_count = int(re.findall(\n",
    "                r'\\d+', elem_blog.text.replace(\",\", \"\"))[0])\n",
    "            self.store_dict['blog_review_count'] = blog_review_count\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict['blog_review_count'] = None\n",
    "\n",
    "        # 도로명주소\n",
    "        print(\"주소\")\n",
    "        try:\n",
    "            address_xpath = \"//strong[contains(.,'주소')]/following-sibling::div/a/span\"\n",
    "            address_elem = self.driver.find_element(By.XPATH, address_xpath)\n",
    "            address_text = address_elem.text\n",
    "            if address_text != \"\":\n",
    "                self.store_dict[\"address\"] = address_text\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict[\"address\"] = None\n",
    "        except Exception as e:\n",
    "            print(\"도로명주소 에러: \", e)\n",
    "            self.store_dict[\"address\"] = False\n",
    "\n",
    "        print(\"전화번호\")\n",
    "        # 매장 전화번호\n",
    "        try:\n",
    "            phone_xpath = \"//strong[contains(.,'전화번호')]/following-sibling::div/span\"\n",
    "            phone_elem = self.driver.find_element(By.XPATH, phone_xpath)\n",
    "            phone_text = phone_elem.text\n",
    "            if phone_text != \"\":\n",
    "                self.store_dict[\"phone\"] = phone_text\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict[\"phone\"] = None\n",
    "        except Exception as e:\n",
    "            print(\"전화번호 에러: \", e)\n",
    "            self.store_dict[\"phone\"] = False\n",
    "\n",
    "        print(\"인스타그램 계정\")\n",
    "\n",
    "        # 인스타그램 계정 존재 확인\n",
    "        try:\n",
    "            elem = self.driver.find_element(\n",
    "                By.XPATH, value=\"//a[contains(@href, 'instagram.com')]\")\n",
    "            instagram_url = elem.get_attribute('href')\n",
    "\n",
    "            result = get_instagram_link(instagram_url)\n",
    "\n",
    "            if result == False:\n",
    "                self.store_dict['instagram_link'] = None\n",
    "                self.store_dict['instagram_post'] = None\n",
    "                self.store_dict['instagram_follower'] = None\n",
    "            elif result != None:\n",
    "                self.store_dict['instagram_link'] = result\n",
    "\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(\"추출 실패\")\n",
    "            self.store_dict['instagram_link'] = None\n",
    "            self.store_dict['instagram_post'] = None\n",
    "            self.store_dict['instagram_follower'] = None\n",
    "        except Exception as e:\n",
    "            print(\"인스타그램 에러:\", e)\n",
    "            self.store_dict['instagram_link'] = None\n",
    "            self.store_dict['instagram_post'] = None\n",
    "            self.store_dict['instagram_follower'] = None\n",
    "\n",
    "        print(\"미쉘린 가이드\")\n",
    "        # 서울 미쉐린 가이드 등재 여부\n",
    "        try:\n",
    "            michelin_xpath = \"\"\"//div[a[contains(text(), '미쉐린 가이드 서울')]]\"\"\"\n",
    "            self.driver.find_element(By.XPATH, michelin_xpath)\n",
    "            self.store_dict['seoul_michelin'] = True\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict['seoul_michelin'] = False\n",
    "        except Exception as e:\n",
    "            print(\"서울 미쉐린 가이드 에러:\", e)\n",
    "            self.store_dict['seoul_michelin'] = None\n",
    "\n",
    "        print(\"역 출구 거리 \")\n",
    "        # 지하철역 출구로부터 거리\n",
    "        try:\n",
    "            subway_xpath = \"/html/body/div[3]/div/div/div/div[5]/div/div[2]/div[1]/div/div[1]/div/div\"\n",
    "            elem = self.driver.find_element(By.XPATH, subway_xpath)\n",
    "            text = elem.text\n",
    "\n",
    "            numbers = re.findall(r'\\d+', text)\n",
    "            if numbers:\n",
    "                self.store_dict[\"distance_from_subway\"] = convert_str_to_number(\n",
    "                    numbers[-1])\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict[\"distance_from_subway\"] = None\n",
    "        except Exception as e:\n",
    "            print(\"지하철역 에러: \", e)\n",
    "            self.store_dict[\"distance_from_subway\"] = None\n",
    "\n",
    "        print(\"방송 출연\")\n",
    "        # 방송 출연 여부\n",
    "        try:\n",
    "            tv_xpath = \"\"\"//strong[descendant::span[text()='TV방송정보']]\"\"\"\n",
    "            self.driver.find_element(By.XPATH, tv_xpath)\n",
    "            self.store_dict['on_tv'] = True\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict['on_tv'] = False\n",
    "        except Exception as e:\n",
    "            print(\"방송 출연 에러: \", e)\n",
    "            self.store_dict['on_tv'] = None\n",
    "\n",
    "        print(\"주차, 노키즈\")\n",
    "        # 주차 가능, 반려동물 동반, 노키즈존\n",
    "        try:\n",
    "            convenient_xpath = \"//strong[descendant::span[text()='편의']]/ancestor::div[1]/div/div\"\n",
    "            elem = self.driver.find_element(By.XPATH, convenient_xpath)\n",
    "            convenients = elem.text\n",
    "\n",
    "            for parking in [\"주차\", \"발렛파킹\"]:\n",
    "                if parking in convenients:\n",
    "                    self.store_dict[\"parking_available\"] = True\n",
    "                    break\n",
    "\n",
    "            if \"노키즈존\" in convenients:\n",
    "                self.store_dict[\"no_kids\"] = True\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict[\"parking_available\"] = False\n",
    "            self.store_dict[\"no_kids\"] = False\n",
    "        except Exception as e:\n",
    "            print(\"주차, 반려동물, 노키즈 에러: \", e)\n",
    "            self.store_dict[\"parking_available\"] = False\n",
    "            self.store_dict[\"no_kids\"] = False\n",
    "\n",
    "        print(\"DataLab\")\n",
    "        # DataLab: 연령별 / 성별 검색 인기도\n",
    "        try:\n",
    "            # entryIframe 스크롤 끝까지 내려서 모든 컨텐츠 로딩\n",
    "            last_height = self.driver.execute_script(\n",
    "                \"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                self.driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.5)\n",
    "                new_height = self.driver.execute_script(\n",
    "                    \"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "            # DataLab 항목 찾고 해당 element로 스크롤 이동\n",
    "            datalab_xpath = \"\"\"//div[h2/span[contains(text(), '데이터랩')]]\"\"\"\n",
    "            datalab_elem = self.driver.find_element(By.XPATH, datalab_xpath)\n",
    "            self.driver.execute_script(\n",
    "                \"arguments[0].scrollIntoView(true);\", datalab_elem)\n",
    "\n",
    "            # \"테마키워드\"라는 text가 있는 경우, \"더보기 버튼을 눌러줘야 연령별/성별 검색어 비율 확인 가능\n",
    "            try:\n",
    "                theme_keyword_xpath = \"\"\".//div/div/div/h3[contains(text(), '테마키워드')]\"\"\"\n",
    "                datalab_elem.find_element(By.XPATH, theme_keyword_xpath)\n",
    "                button_elem = datalab_elem.find_element(\n",
    "                    By.XPATH, \".//div[2]/div/a\")\n",
    "                self.driver.execute_script(\"arguments[0].click()\", button_elem)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(\"데이터랩 더보기 버튼 에러: \", e)\n",
    "\n",
    "            # 20대와 30대가 top 1, 2를 차지하는지 확인\n",
    "            age_xpath = \"\"\"//*[@id=\"bar_chart_container\"]/ul/li/div[1]/span/span[1]\"\"\"\n",
    "            age_elements = self.wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, age_xpath)))\n",
    "            percentage_by_age = [\n",
    "                round(float(item.text.replace('%', '')), 2) for item in age_elements]\n",
    "            top_two = sorted(percentage_by_age, reverse=True)[:2]\n",
    "            is_2030_in_top_two = percentage_by_age[1] in top_two and percentage_by_age[2] in top_two\n",
    "            if is_2030_in_top_two:\n",
    "                self.store_dict[\"age-2030\"] = True\n",
    "            else:\n",
    "                self.store_dict[\"age-2030\"] = False\n",
    "\n",
    "            # 남성의 비율이 50%를 넘는지 확인\n",
    "            gender_xpath = \"\"\"//*[@id=\"pie_chart_container\"]/div/*[local-name()='svg']/*[local-name()='g'][1]/*[local-name()='g'][3]/*[local-name()='g'][4]/*[local-name()='g']/*[local-name()='text'][2]\"\"\"\n",
    "            gender_elements = self.wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, gender_xpath)))\n",
    "            female, male = [round(float(item.text.replace(\"%\", \"\")), 0)\n",
    "                            for item in gender_elements]\n",
    "            if male > 50:\n",
    "                self.store_dict[\"gender-balance\"] = False\n",
    "            else:\n",
    "                self.store_dict[\"gender-balance\"] = True\n",
    "        except NoSuchElementException:\n",
    "            self.store_dict[\"age-2030\"] = None\n",
    "            self.store_dict[\"gender-balance\"] = None\n",
    "        except Exception as e:\n",
    "            print(\"데이터랩 에러: \", e)\n",
    "            self.store_dict[\"age-2030\"] = None\n",
    "            self.store_dict[\"gender-balance\"] = None\n",
    "\n",
    "        print(\"블루리본서베이\")\n",
    "        # 블루 리본 등재 여부\n",
    "        if self.is_food == True:\n",
    "            if self.store_dict[\"name\"].replace(\" \", \"\") in [name.replace(\" \", \"\") for name in self.blue_ribbon[\"name\"].values]:\n",
    "                indices = self.blue_ribbon.index[self.blue_ribbon[\"name\"]\n",
    "                                                 == self.store_dict[\"name\"].replace(\" \", \"\")].tolist()\n",
    "\n",
    "                for i, index in enumerate(indices):\n",
    "                    # 1. 도로명 주소 비교\n",
    "                    # - 띄어쓰기 모두 제거한 상태로 비교\n",
    "                    address1 = self.store_dict[\"address\"].replace(\" \", \"\")\n",
    "                    address2 = self.blue_ribbon[\"address\"][index].replace(\n",
    "                        \" \", \"\")\n",
    "\n",
    "                    if address1 == address2:\n",
    "                        self.store_dict[\"on_blue_ribbon\"] = True\n",
    "                        break\n",
    "                    # 2. 위도-경도 비교\n",
    "                    lat1 = float(self.store_dict[\"gps\"][\"latitude\"])\n",
    "                    lon1 = float(self.store_dict[\"gps\"][\"longitude\"])\n",
    "\n",
    "                    lat2 = float(self.blue_ribbon[\"latitude\"][index])\n",
    "                    lon2 = float(self.blue_ribbon[\"longitude\"][index])\n",
    "                    distance = haversine(lat1, lon1, lat2, lon2)\n",
    "                    if distance <= 50:\n",
    "                        self.store_dict[\"on_blue_ribbon\"] = True\n",
    "                        break\n",
    "                    if i + 1 == len(indices):\n",
    "                        self.store_dict[\"on_blue_ribbon\"] = False\n",
    "                        break\n",
    "            elif self.store_dict[\"phone\"] in [phone for phone in self.blue_ribbon[\"phone\"].values]:\n",
    "                self.store_dict[\"on_blue_ribbon\"] = True\n",
    "            else:\n",
    "                self.store_dict[\"on_blue_ribbon\"] = False\n",
    "        else:\n",
    "            self.store_dict['on_blue_ribbin'] = False\n",
    "\n",
    "        # 방문자 리뷰 작성 일자 크롤링\n",
    "        print(\"최근 영업 상태\")\n",
    "        try:\n",
    "            self.store_dict[\"running_well\"] = 1\n",
    "            self.move_to_tab('리뷰')\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            latest_xpath = \"\"\"//a[@role='option' and text()='최신순']\"\"\"\n",
    "            elem = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, latest_xpath)))\n",
    "            self.driver.execute_script(\"arguments[0].click()\", elem)\n",
    "\n",
    "            date_xpath = \"\"\"//div[@class='place_section_content']/ul/li//span[contains(text(), '방문일')]/following-sibling::span\"\"\"\n",
    "            elements = self.wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, date_xpath)))\n",
    "            date_texts = [elem.text for elem in elements]\n",
    "\n",
    "            for date_text in date_texts:\n",
    "                if not is_within_one_month(date_text):\n",
    "                    self.store_dict[\"running_well\"] = 0\n",
    "                    break\n",
    "\n",
    "            if self.store_dict['running_well'] == 1:\n",
    "                self.store_dict['running_well'] = 2\n",
    "                for date_text in date_texts:\n",
    "                    if not is_within_two_weeks(date_text):\n",
    "                        self.store_dict[\"running_well\"] = 1\n",
    "                        break\n",
    "        except NoSuchElementException:\n",
    "            # 리뷰탭이 없거나, 리뷰가 없는 경우\n",
    "            self.store_dict[\"running_well\"] = 1\n",
    "        except Exception as e:\n",
    "            print(\"최근 영업 상태에서 예상치 못한 에러 발생\")\n",
    "            print(\"이슈 내용: \", e)\n",
    "            self.store_dict[\"running_well\"] = 1\n",
    "\n",
    "        print(\"사진 Url\")\n",
    "        # 대표사진 크롤링\n",
    "        self.store_dict[\"image_urls\"] = []\n",
    "        # try:\n",
    "        #     self.store_dict[\"image_urls\"] = []\n",
    "\n",
    "        #     self.move_to_tab(\"사진\")\n",
    "        #     self.store_dict[\"image_urls\"] = self.get_image_urls()\n",
    "        # except NoSuchElementException as e:\n",
    "        #     print(\"사진 탭 없음\")\n",
    "        #     self.store_dict[\"image_urls\"] = []\n",
    "\n",
    "        print(\"인스타그램 팔로워, 게시글\")\n",
    "        if self.store_dict['instagram_link'] != None:  # 인스타그램 계정이 있는 경우에만 실행\n",
    "            try:\n",
    "                instagram_embed_url = self.store_dict['instagram_link'] + \"/embed\"\n",
    "\n",
    "                # 인스타그랩 탭으로 이동\n",
    "                self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "                self.driver.get(instagram_embed_url)\n",
    "                time.sleep(1)\n",
    "\n",
    "                name_xpath = \"\"\"/html/body/div/div/div/div/div/div/div/div/div[1]/div[2]/div[1]/a/div\"\"\"\n",
    "                self.wait.until(EC.presence_of_element_located(\n",
    "                    (By.XPATH, name_xpath)))\n",
    "\n",
    "                # xpath = \"\"\"//div[contains(@class, 'EmbedProfile')]//span[contains(text(), '팔로워 ') and contains(text(), '게시물 ')]/span/span\"\"\"\n",
    "                # elements = self.driver.find_elements(By.XPATH, xpath)\n",
    "\n",
    "                # follower_xpath = \"\"\"//span[contains(., '팔로워 ')]/span/span\"\"\"\n",
    "                follower_xpath = \"\"\"/html/body/div/div/div/div/div/div/div/div/div[1]/div[2]/div[3]/span/span[1]/span\"\"\"\n",
    "                # post_xpath = \"\"\"//span[contains(., '게시물 ')]/span/span\"\"\"\n",
    "                post_xpath = \"\"\"/html/body/div/div/div/div/div/div/div/div/div[1]/div[2]/div[3]/span/span[2]/span\"\"\"\n",
    "\n",
    "                follower_elem = self.driver.find_element(\n",
    "                    By.XPATH, follower_xpath)\n",
    "                post_elem = self.driver.find_element(By.XPATH, post_xpath)\n",
    "\n",
    "                follower = convert_str_to_number(follower_elem.text)\n",
    "                post = convert_str_to_number(post_elem.text)\n",
    "                self.store_dict[\"instagram_follower\"] = follower\n",
    "                self.store_dict[\"instagram_post\"] = post\n",
    "            except (NoSuchElementException, TimeoutException, WebDriverException) as e:\n",
    "                print(\"인스타그램 크롤링 문제\")\n",
    "                print(e)\n",
    "                self.store_dict['instagram_link'] = None\n",
    "                self.store_dict[\"instagram_follower\"] = None\n",
    "                self.store_dict[\"instagram_post\"] = None\n",
    "\n",
    "            # 네이버지도 탭으로 복귀\n",
    "            self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "        # 한 매장에 대한 크롤링 결과\n",
    "        print(self.store_dict)\n",
    "        self.insert_into_dataframe()\n",
    "\n",
    "    # 한 매장에 대한 크롤링 정볼르 DataFrame에 Insertion\n",
    "    def insert_into_dataframe(self):\n",
    "        new_data = pd.DataFrame([self.store_dict])\n",
    "        self.data = pd.concat([self.data, new_data], ignore_index=True)\n",
    "\n",
    "    # 한 페이지 크롤링\n",
    "    def crawling_one_page(self):\n",
    "        self.move_to_search_iframe()\n",
    "        store_count = self.scroll_to_end()\n",
    "\n",
    "        for i in range(1, store_count + 1):\n",
    "\n",
    "            print(f\"==== {i} 번째 매장 ====\")\n",
    "            if self.get_into_store(index=i) == False:\n",
    "                continue\n",
    "            self.get_store_details()\n",
    "        self.driver.execute_cdp_cmd(\"Network.clearBrowserCache\", {})\n",
    "        self.driver.execute_cdp_cmd(\"Network.clearBrowserCookies\", {})\n",
    "\n",
    "    # 한 페이지에 대한 매장 개수 반환\n",
    "    def scroll_to_end(self):\n",
    "        try:\n",
    "            li_xpath = \"\"\"//*[@id=\"_pcmap_list_scroll_container\"]/ul/li\"\"\"\n",
    "            store_elements = self.wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, li_xpath)))\n",
    "            store_count = len(store_elements)\n",
    "            self.driver.execute_script(\n",
    "                \"arguments[0].scrollIntoView(true);\", store_elements[-1])\n",
    "\n",
    "            while True:\n",
    "                time.sleep(0.5)\n",
    "                store_elements = self.wait.until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, li_xpath)))\n",
    "                new_store_count = len(store_elements)\n",
    "\n",
    "                if store_count == new_store_count:\n",
    "                    break\n",
    "                store_count = new_store_count\n",
    "                self.driver.execute_script(\n",
    "                    \"arguments[0].scrollIntoView(true);\", store_elements[-1])\n",
    "            return store_count\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"매장 정보를 찾을 수 없습니다.\")\n",
    "            return 0\n",
    "\n",
    "    # searchIframe으로 이동\n",
    "    def move_to_search_iframe(self):\n",
    "        self.move_to_default_content()\n",
    "        self.wait.until(EC.frame_to_be_available_and_switch_to_it(\n",
    "            (By.XPATH, self.search_iframe)))\n",
    "        self.wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, self.empty_searchIframe)))\n",
    "\n",
    "    # 다음 페이지로 이동\n",
    "    def move_to_next_page(self):\n",
    "        self.driver.switch_to.default_content()\n",
    "        self.wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, self.empty_root)))\n",
    "        self.wait.until(EC.frame_to_be_available_and_switch_to_it(\n",
    "            (By.XPATH, self.search_iframe)))\n",
    "\n",
    "        nextpage_xpath = \"\"\"//a[span[contains(text(),'다음페이지')]]\"\"\"\n",
    "        next_page_button = self.wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, nextpage_xpath)))\n",
    "\n",
    "        # 다음페이지 존재 여부 확인\n",
    "        aria_disabled = next_page_button.get_attribute(\"aria-disabled\")\n",
    "        if aria_disabled == \"true\":\n",
    "            return False\n",
    "        else:\n",
    "            next_page_button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "\n",
    "    def crawl_popular_menu(self):\n",
    "\n",
    "        self.click_filter_button()\n",
    "\n",
    "        # 메뉴 종류 긁어오기\n",
    "        menu_list_xpath = \"\"\"//div[@id='modal-root']//div[@id='_popup_menu']/following-sibling::div//span/a\"\"\"\n",
    "        self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, menu_list_xpath)))\n",
    "        elements = self.driver.find_elements(By.XPATH, menu_list_xpath)\n",
    "        menu_list = [element.text for element in elements].copy()\n",
    "\n",
    "        # 각 메뉴에 대해 크롤링 진행\n",
    "        for index, menu in enumerate(menu_list):\n",
    "            if index != 0:\n",
    "                self.click_filter_button()\n",
    "\n",
    "            if self.click_menu_button(menu) == False:\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "\n",
    "            for page in range(1, 7):\n",
    "                print(\"=\"*10+f\"page {page}\" + \"=\"*10)\n",
    "                self.crawling_one_page()\n",
    "                print(self.data)\n",
    "                # 마지막 페이지인 경우\n",
    "                if self.move_to_next_page() == False:\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "\n",
    "    # searchIframe의 필터 버튼 클릭\n",
    "    def click_filter_button(self):\n",
    "        time.sleep(1)\n",
    "        self.move_to_search_iframe()\n",
    "        # \"더보기\" 버튼 클릭\n",
    "        filter_xpath = \"\"\"//a[span[contains(text(),'전체필터')]]\"\"\"\n",
    "        filtter_button = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, filter_xpath)))\n",
    "        self.driver.execute_script(\"arguments[0].click()\", filtter_button)\n",
    "\n",
    "    # 메뉴 클릭하고 \"결과보기\" 버튼 클릭\n",
    "    def click_menu_button(self, menu_text):\n",
    "        print(\"=\"*15 + f\"{menu_text} 크롤링\" + \"=\"*15)\n",
    "        menu_xpath = f\"\"\"//div[@id='modal-root']//div[@id='_popup_menu']/following-sibling::div//span/a[text()='{\n",
    "            menu_text}']\"\"\"\n",
    "\n",
    "        menu_item = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, menu_xpath)))\n",
    "\n",
    "        self.driver.execute_script(\"arguments[0].click()\", menu_item)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        submit_xpath = f\"\"\"//div[@id='modal-root']//a[contains(text(), '결과보기 ')]\"\"\"\n",
    "        submit_button = self.driver.find_element(By.XPATH, submit_xpath)\n",
    "\n",
    "        # 검색 결과 0건일 경우, aria-disabled = 'true'임\n",
    "        # 0건이 아닌 경우 True 반환\n",
    "        if submit_button.get_attribute('aria-disabled') == 'false':\n",
    "            self.driver.execute_script(\"arguments[0].click()\", submit_button)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def crawling(self):\n",
    "        self.search_keyword()\n",
    "        if self.crawl_new:\n",
    "            self.click_new_option()\n",
    "        for page in range(1, 7):\n",
    "            print(\"=\"*10+f\"page {page}\" + \"=\"*10)\n",
    "            self.crawling_one_page()\n",
    "            # 마지막 페이지인 경우\n",
    "            if self.move_to_next_page() == False:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "        if self.keyword == \"맛집\" and not self.crawl_new:\n",
    "            self.crawl_popular_menu()\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 시각\n",
      "2024-02-07 10:06:52.857147\n",
      "연남동 맛집  크롤링 시작\n",
      "==========page 1==========\n",
      "==== 1 번째 매장 ====\n",
      "핫스팟 계산\n",
      "3초 대기\n",
      "3초 끝\n",
      "매장 이름\n",
      "신규매장 여부 확인\n",
      "네이버 리뷰\n",
      "주소\n",
      "전화번호\n",
      "인스타그램 계정\n",
      "인스타링크 추출\n",
      "https://www.instagram.com/etna_piu\n",
      "미쉘린 가이드\n",
      "역 출구 거리 \n",
      "방송 출연\n",
      "주차, 노키즈\n",
      "DataLab\n",
      "블루리본서베이\n",
      "최근 영업 상태\n",
      "사진 Url\n",
      "인스타그램 팔로워, 게시글\n",
      "https://www.instagram.com/etna_piu\n",
      "인스타탭으로 이동\n",
      "https://www.instagram.com/etna_piu/embed\n",
      "{'store_id': '1922159413', 'name': '에트나퓨', 'category': '이탈리아음식', 'is_food': True, 'new_store': False, 'instagram_link': 'https://www.instagram.com/etna_piu', 'instagram_post': 47, 'instagram_follower': 2200, 'hot_spot': True, 'visitor_review_count': 537, 'blog_review_count': 391, 'distance_from_subway': 462, 'on_tv': True, 'parking_available': False, 'no_kids': False, 'seoul_michelin': False, 'age-2030': None, 'gender-balance': None, 'on_blue_ribbon': False, 'running_well': 0, 'address': '서울 마포구 동교로38안길 3 2층', 'phone': '0507-1408-0517', 'gps': {'latitude': 37.5619444, 'longitude': 126.9246293}, 'naver_url': 'https://pcmap.place.naver.com/place/1922159413?entry=bmp&n_ad_group_type=10&n_query=%EC%97%B0%EB%82%A8%EB%8F%99%EB%A7%9B%EC%A7%91&from=map&fromPanelNum=2&x=126.9246293&y=37.5619444&timestamp=202402071007', 'image_urls': []}\n",
      "==== 2 번째 매장 ====\n",
      "핫스팟 계산\n",
      "3초 대기\n",
      "3초 끝\n",
      "매장 이름\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     crawler \u001b[38;5;241m=\u001b[39m DatePopCrawler(location\u001b[38;5;241m=\u001b[39mlocation, keyword\u001b[38;5;241m=\u001b[39mkeyword, hotspots\u001b[38;5;241m=\u001b[39mseoul_hotspots,\n\u001b[1;32m     56\u001b[0m                         is_food\u001b[38;5;241m=\u001b[39mis_food, crawl_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, blue_ribbon\u001b[38;5;241m=\u001b[39mbluer_data)\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mcrawler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidSessionIdException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "Cell \u001b[0;32mIn[3], line 807\u001b[0m, in \u001b[0;36mDatePopCrawler.crawling\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawling_one_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# 마지막 페이지인 경우\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_to_next_page() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 679\u001b[0m, in \u001b[0;36mDatePopCrawler.crawling_one_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_into_store(index\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_store_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mexecute_cdp_cmd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork.clearBrowserCache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mexecute_cdp_cmd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork.clearBrowserCookies\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[3], line 295\u001b[0m, in \u001b[0;36mDatePopCrawler.get_store_details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m     store_name_xpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_title\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/div/span\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located(\n\u001b[1;32m    293\u001b[0m         (By\u001b[38;5;241m.\u001b[39mXPATH, store_name_xpath)))\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43melem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m elem[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# 드물게 매장 이름을 크롤링하지 못하는 에러 발생\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# 해당 매장 생략하고 다음 매장 크롤링 진행\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webelement.py:90\u001b[0m, in \u001b[0;36mWebElement.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The text of the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_ELEMENT_TEXT\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/_request_methods.py:136\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m     urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m body\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    145\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/_request_methods.py:183\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[1;32m    181\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/eagle-eye-XOKA1mmI-py3.12/lib/python3.12/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1419\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1419\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"지금 시각\")\n",
    "    print(datetime.now())\n",
    "\n",
    "    crawling_dict_list = [\n",
    "        {\n",
    "            \"location\": \"강남역\",\n",
    "            \"keyword\": [\"맛집\", \"공방\", \"만화카페\", \"커플 스튜디오\", \"동물카페\"]\n",
    "        },\n",
    "        {\n",
    "            \"location\": \"가로수길\",\n",
    "            \"keyword\": [\"맛집\", \"공방\", \"만화카페\", \"커플 스튜디오\", \"동물카페\"]\n",
    "        },\n",
    "        {\n",
    "            \"location\": \"대학로\",\n",
    "            \"keyword\": [\"맛집\", \"공방\", \"만화카페\", \"커플 스튜디오\", \"동물카페\", \"연극\"]\n",
    "        },\n",
    "        {\n",
    "            \"location\": \"홍대\",\n",
    "            \"keyword\": [\"맛집\", \"공방\", \"만화카페\", \"커플 스튜디오\", \"동물카페\", \"연극\"]\n",
    "        },\n",
    "        {\n",
    "            \"location\": \"연남동\",\n",
    "            \"keyword\": [\"맛집\", \"공방\", \"만화카페\", \"커플 스튜디오\", \"동물카페\"]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    curr_crawling = crawling_dict_list[4]\n",
    "\n",
    "    keywords = curr_crawling['keyword']\n",
    "    location = curr_crawling['location']\n",
    "\n",
    "    seoul_hotspots = load_hotspots('seoul_hotspots.csv')\n",
    "\n",
    "    for keyword in keywords:\n",
    "\n",
    "        try:\n",
    "            search_word = location + \" \" + keyword\n",
    "            print(search_word, \" 크롤링 시작\")\n",
    "\n",
    "            # 음식 크롤링 여부\n",
    "            is_food = False\n",
    "            bluer_data = None\n",
    "            if keyword == \"맛집\":\n",
    "                is_food = True\n",
    "                bluer_data1 = pd.read_csv(\"data/blueribbon/서울 강남_bluer.csv\", header=0)\n",
    "                bluer_data2 = pd.read_csv(\"data/blueribbon/서울 강북_bluer.csv\", header=0)\n",
    "                bluer_data = pd.concat([bluer_data1, bluer_data2], ignore_index=True)\n",
    "\n",
    "                del bluer_data1\n",
    "                del bluer_data2\n",
    "\n",
    "            try:\n",
    "                crawler = DatePopCrawler(location=location, keyword=keyword, hotspots=seoul_hotspots,\n",
    "                                    is_food=is_food, crawl_new=False, blue_ribbon=bluer_data)\n",
    "                crawler.crawling()\n",
    "            except InvalidSessionIdException as e:\n",
    "                print(e)\n",
    "                crawler = DatePopCrawler(location=location, keyword=keyword, hotspots=seoul_hotspots,\n",
    "                                    is_food=is_food, crawl_new=False, blue_ribbon=bluer_data)\n",
    "                crawler.crawling()\n",
    "            \n",
    "            crawler_data_unique = crawler.data.drop_duplicates(\n",
    "                subset='store_id', keep='first')\n",
    "\n",
    "            if keyword == \"맛집\":\n",
    "\n",
    "                print(\"Start to crawl new stores\")\n",
    "                # 동일 검색어로 신규오픈 매장 크롤링\n",
    "                crawler_new = DatePopCrawler(location=location, keyword=keyword, hotspots=seoul_hotspots,\n",
    "                                            is_food=is_food, crawl_new=True, blue_ribbon=bluer_data)\n",
    "                crawler_new.crawling()\n",
    "                crawler_new_data_unique = crawler_new.data.drop_duplicates(\n",
    "                    subset='store_id', keep='first')\n",
    "                print(f\"Crawling for {location} {keyword} is done.\")\n",
    "                merged_data = pd.concat([crawler_data_unique, crawler_new_data_unique])\n",
    "                merged_data = merged_data.drop_duplicates(subset='store_id', keep='first')   \n",
    "            else:\n",
    "                merged_data = crawler_data_unique\n",
    "\n",
    "            merged_data.reset_index(inplace=True)\n",
    "            merged_data.drop(columns=\"index\", inplace=True)\n",
    "\n",
    "            print(f\"merge하고 중복 제거 후 개수: {len(merged_data)}\")\n",
    "\n",
    "            merged_data.to_csv(f'data/crawl_result/{location}{keyword}.csv', index=True, encoding='utf-8-sig')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    print(\"끝난 시각\")\n",
    "    print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'store_id': '33752484',\n",
       " 'name': '',\n",
       " 'category': '',\n",
       " 'is_food': False,\n",
       " 'new_store': False,\n",
       " 'instagram_link': None,\n",
       " 'instagram_post': None,\n",
       " 'instagram_follower': None,\n",
       " 'hot_spot': False,\n",
       " 'visitor_review_count': 0,\n",
       " 'blog_review_count': 0,\n",
       " 'distance_from_subway': None,\n",
       " 'on_tv': False,\n",
       " 'parking_available': False,\n",
       " 'no_kids': False,\n",
       " 'seoul_michelin': False,\n",
       " 'age-2030': None,\n",
       " 'gender-balance': None,\n",
       " 'on_blue_ribbon': None,\n",
       " 'running_well': None,\n",
       " 'address': None,\n",
       " 'phone': None,\n",
       " 'gps': {'latitude': 37.4895192, 'longitude': 127.0198051}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.store_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawler_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcrawler_new\u001b[49m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawler_new' is not defined"
     ]
    }
   ],
   "source": [
    "crawler_new.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry (eagle-eye)",
   "language": "python",
   "name": "eagle-eye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
